[env]
exp_dir = ~/haowen/GitHub/autoLoss
data_dir = ${exp_dir}/Data
model_dir = /datasets/BigLearning/haowen/autoLoss/saved_models_rebuttal

[data]
train_data_file = reg_16_200/train.npy
valid_data_file = reg_16_200/valid.npy
train_stud_data_file = reg_16_200/train_stud.npy
#train_data_file = reg_16_200_4/train.npy
#valid_data_file = reg_16_200_4/valid.npy
#train_stud_data_file = reg_16_200_4/train_stud.npy
num_sample_valid = 200
num_sample_train = 200
num_sample_train_stud = 200
mean_noise = 0
var_noise = 2

[stud]
student_model_name = reg
batch_size = 200
dim_input_stud = 16
dim_hidden_stud = 64
dim_output_stud = 1
lr_stud = 0.0005
valid_frequency_stud = 10
max_endurance_stud = 100
max_training_step = 10000
lambda1_stud = 0.4
lambda2_stud = 0.05

[train]

[evaluate]

[rl]
controller_model_name = 2layer_logits_clipping
num_pre_loss = 2
dim_state_rl = 5
dim_hidden_rl = 16
dim_action_rl = 2
# adam
lr_rl = 0.001
# sdg
#lr_rl = 0.1
lr_decay_rl = 1
total_episodes = 400
update_frequency = 1
save_frequency = 50
# according to ENAS code, this is very important
reward_baseline_decay = 0.8
reward_c = 20000
# Set an max step reward, in case the improve baseline is too small and cause
# huge reward.
reward_max_value = 20
explore_rate_decay_rl = 50
explore_rate_rl = 0
reward_step_rl = 0.1
max_endurance_rl = 50
logit_clipping_c = 2
optimizer_ctrl = adam
