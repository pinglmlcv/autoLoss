2018-05-05 23:30:56.043892: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-05-05 23:30:56.962077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:03:00.0
totalMemory: 11.92GiB freeMemory: 11.80GiB
2018-05-05 23:30:56.962107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0, compute capability: 5.2)
WARNING::lambda1_stud not found in config file
WARNING::lambda2_stud not found in config file
2018-05-05 23:30:57.042088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0, compute capability: 5.2)
2018-05-05 23:30:57.154966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0, compute capability: 5.2)
/users/hzhang2/haowen/miniconda3/envs/py35_tf/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
INFO::=================
INFO::episodes: 0
INFO::----step1000----
INFO::hq_ratio: 0.04
INFO::entropy: 2.6214161361850623
INFO::----step2000----
INFO::hq_ratio: 0.0359
INFO::entropy: 2.5820973956789484
INFO::----step3000----
INFO::hq_ratio: 0.046
INFO::entropy: 1.2141119564910197
INFO::----step4000----
INFO::hq_ratio: 0.0198
INFO::entropy: 0.8867271898614716
INFO::----step5000----
INFO::hq_ratio: 0.0149
INFO::entropy: 0.2600738620696318
INFO::----step6000----
INFO::hq_ratio: 0.0044
INFO::entropy: 0.8454692975107967
INFO::----step7000----
INFO::hq_ratio: 0.0247
INFO::entropy: 0.02634986423007532
INFO::----step8000----
INFO::hq_ratio: 0.0032
INFO::entropy: 4.410963378559342e-06
INFO::----step9000----
INFO::hq_ratio: 0.0234
INFO::entropy: 0.16189667713800196
INFO::----step10000----
INFO::hq_ratio: 0.1173
INFO::entropy: 4.410963378559342e-06
INFO::----step11000----
INFO::hq_ratio: 0.0525
INFO::entropy: 1.8516758558380029
INFO::----step12000----
INFO::hq_ratio: 0.0486
INFO::entropy: 2.1473315913835185
INFO::----step13000----
INFO::hq_ratio: 0.0625
INFO::entropy: 2.2470108866351914
INFO::----step14000----
INFO::hq_ratio: 0.0499
INFO::entropy: 1.7594861742110772
INFO::----step15000----
INFO::hq_ratio: 0.0381
INFO::entropy: 2.113011177337694
INFO::----step16000----
INFO::hq_ratio: 0.0792
INFO::entropy: 2.8325658030177654
INFO::----step17000----
INFO::hq_ratio: 0.1149
INFO::entropy: 2.719805577333406
INFO::----step18000----
INFO::hq_ratio: 0.1447
INFO::entropy: 2.722960656125714
INFO::----step19000----
INFO::hq_ratio: 0.1804
INFO::entropy: 2.720316946954177
INFO::----step20000----
INFO::hq_ratio: 0.1683
INFO::entropy: 2.7990818598861007
INFO::----step21000----
INFO::hq_ratio: 0.2343
INFO::entropy: 2.890327410277319
INFO::----step22000----
INFO::hq_ratio: 0.2098
INFO::entropy: 2.891428705981359
INFO::----step23000----
INFO::hq_ratio: 0.2006
INFO::entropy: 2.727272506085979
INFO::----step24000----
INFO::hq_ratio: 0.2827
INFO::entropy: 2.8713100947829537
INFO::----step25000----
INFO::hq_ratio: 0.251
INFO::entropy: 2.8602673666563443
INFO::----step26000----
INFO::hq_ratio: 0.3327
INFO::entropy: 2.835546129434505
INFO::----step27000----
INFO::hq_ratio: 0.3158
INFO::entropy: 2.8101048018156365
INFO::----step28000----
INFO::hq_ratio: 0.3134
INFO::entropy: 2.784865917200337
INFO::----step29000----
INFO::hq_ratio: 0.3404
INFO::entropy: 2.8083801436359592
INFO::----step30000----
INFO::hq_ratio: 0.4044
INFO::entropy: 2.846641722564417
INFO::----step31000----
INFO::hq_ratio: 0.4089
INFO::entropy: 2.8412992510040946
INFO::----step32000----
INFO::hq_ratio: 0.3707
INFO::entropy: 2.701807702648159
INFO::----step33000----
INFO::hq_ratio: 0.4128
INFO::entropy: 2.75453332636683
INFO::----step34000----
INFO::hq_ratio: 0.3778
INFO::entropy: 2.7457695052363578
INFO::----step35000----
INFO::hq_ratio: 0.4376
INFO::entropy: 2.7815943417738485
INFO::----step36000----
INFO::hq_ratio: 0.4409
INFO::entropy: 2.790544889405916
INFO::----step37000----
INFO::hq_ratio: 0.3552
INFO::entropy: 2.7134442361016293
INFO::----step38000----
INFO::hq_ratio: 0.3939
INFO::entropy: 2.654837800335189
INFO::----step39000----
INFO::hq_ratio: 0.4423
INFO::entropy: 2.7351073670757473
INFO::----step40000----
INFO::hq_ratio: 0.4289
INFO::entropy: 2.742865061383657
INFO::----step41000----
INFO::hq_ratio: 0.4772
INFO::entropy: 2.7852473259593102
INFO::----step42000----
INFO::hq_ratio: 0.4628
INFO::entropy: 2.7427619692528165
INFO::----step43000----
INFO::hq_ratio: 0.4006
INFO::entropy: 2.666818347641137
INFO::----step44000----
INFO::hq_ratio: 0.455
INFO::entropy: 2.702754997776669
INFO::----step45000----
INFO::hq_ratio: 0.5033
INFO::entropy: 2.7627425409693265
INFO::----step46000----
INFO::hq_ratio: 0.5721
INFO::entropy: 2.7376238662093915
INFO::----step47000----
INFO::hq_ratio: 0.4828
INFO::entropy: 2.6777895946642922
INFO::----step48000----
INFO::hq_ratio: 0.5053
INFO::entropy: 2.6985938902131377
INFO::----step49000----
INFO::hq_ratio: 0.5083
INFO::entropy: 2.722523677150986
INFO::----step50000----
INFO::hq_ratio: 0.5125
INFO::entropy: 2.756417322975132
INFO::----step51000----
INFO::hq_ratio: 0.4793
INFO::entropy: 2.6961443846856157
INFO::----step52000----
INFO::hq_ratio: 0.5593
INFO::entropy: 2.6940522680647354
INFO::----step53000----
INFO::hq_ratio: 0.5763
INFO::entropy: 2.754539558653896
INFO::----step54000----
INFO::hq_ratio: 0.5329
INFO::entropy: 2.7320106918903946
INFO::----step55000----
INFO::hq_ratio: 0.5256
INFO::entropy: 2.687615485808245
INFO::----step56000----
INFO::hq_ratio: 0.5939
INFO::entropy: 2.7339629624999855
INFO::----step57000----
INFO::hq_ratio: 0.5825
INFO::entropy: 2.7222560560282614
INFO::----step58000----
INFO::hq_ratio: 0.5144
INFO::entropy: 2.7146518019061996
INFO::----step59000----
INFO::hq_ratio: 0.5764
INFO::entropy: 2.6212775526824905
INFO::----step60000----
INFO::hq_ratio: 0.6217
INFO::entropy: 2.738985001854064
INFO::----step61000----
INFO::hq_ratio: 0.619
INFO::entropy: 2.7212186006747667
INFO::----step62000----
INFO::hq_ratio: 0.6192
INFO::entropy: 2.736213218630625
INFO::----step63000----
INFO::hq_ratio: 0.6059
INFO::entropy: 2.7213077660501686
INFO::----step64000----
INFO::hq_ratio: 0.5558
INFO::entropy: 2.711427114707279
INFO::----step65000----
INFO::hq_ratio: 0.6019
INFO::entropy: 2.675120809717184
INFO::----step66000----
INFO::hq_ratio: 0.5857
INFO::entropy: 2.6878925577622748
INFO::----step67000----
INFO::hq_ratio: 0.5966
INFO::entropy: 2.7249825337186158
INFO::----step68000----
INFO::hq_ratio: 0.5554
INFO::entropy: 2.629913092288703
INFO::----step69000----
INFO::hq_ratio: 0.6097
INFO::entropy: 2.6761758626978382
INFO::----step70000----
INFO::hq_ratio: 0.5423
INFO::entropy: 2.612227640640186
INFO::----step71000----
INFO::hq_ratio: 0.6464
INFO::entropy: 2.712807828623124
INFO::----step72000----
INFO::hq_ratio: 0.6056
INFO::entropy: 2.7228933110833444
INFO::----step73000----
INFO::hq_ratio: 0.6523
INFO::entropy: 2.7037464194139624
INFO::----step74000----
INFO::hq_ratio: 0.6798
INFO::entropy: 2.699797636343714
INFO::----step75000----
INFO::hq_ratio: 0.5145
INFO::entropy: 2.6227498689921473
INFO::----step76000----
INFO::hq_ratio: 0.6764
INFO::entropy: 2.716312339113934
INFO::----step77000----
INFO::hq_ratio: 0.6491
INFO::entropy: 2.7215240153407825
INFO::----step78000----
INFO::hq_ratio: 0.5919
INFO::entropy: 2.682952387701103
INFO::----step79000----
INFO::hq_ratio: 0.6573
INFO::entropy: 2.7243836082206236
INFO::----step80000----
INFO::hq_ratio: 0.6052
INFO::entropy: 2.680179958000257
INFO::----step81000----
INFO::hq_ratio: 0.6777
INFO::entropy: 2.6969721771639663
INFO::----step82000----
INFO::hq_ratio: 0.6959
INFO::entropy: 2.723893217350126
INFO::----step83000----
INFO::hq_ratio: 0.6832
INFO::entropy: 2.7452713169027367
INFO::----step84000----
INFO::hq_ratio: 0.6692
INFO::entropy: 2.744971394254166
INFO::----step85000----
INFO::hq_ratio: 0.581
INFO::entropy: 2.6602858916827428
INFO::----step86000----
INFO::hq_ratio: 0.7131
INFO::entropy: 2.746790529516712
INFO::----step87000----
INFO::hq_ratio: 0.6966
INFO::entropy: 2.7397397871482414
INFO::----step88000----
INFO::hq_ratio: 0.6942
INFO::entropy: 2.7076095305015753
INFO::----step89000----
INFO::hq_ratio: 0.7012
INFO::entropy: 2.7144611829031837
INFO::----step90000----
INFO::hq_ratio: 0.7033
INFO::entropy: 2.7503203205628806
INFO::----step91000----
INFO::hq_ratio: 0.7058
INFO::entropy: 2.730666614984745
INFO::----step92000----
INFO::hq_ratio: 0.7137
INFO::entropy: 2.7149359115485345
INFO::----step93000----
INFO::hq_ratio: 0.7322
INFO::entropy: 2.729850601437271
INFO::----step94000----
INFO::hq_ratio: 0.6891
INFO::entropy: 2.7474719530531857
INFO::----step95000----
INFO::hq_ratio: 0.7211
INFO::entropy: 2.7350584918675924
INFO::----step96000----
INFO::hq_ratio: 0.6876
INFO::entropy: 2.7450093349745983
INFO::----step97000----
INFO::hq_ratio: 0.7173
INFO::entropy: 2.7489859281976408
INFO::----step98000----
INFO::hq_ratio: 0.6163
INFO::entropy: 2.686868587662127
INFO::----step99000----
INFO::hq_ratio: 0.689
INFO::entropy: 2.736838016316165
INFO::----step100000----
INFO::hq_ratio: 0.7172
INFO::entropy: 2.7416036443526717
INFO::----step101000----
INFO::hq_ratio: 0.7147
INFO::entropy: 2.749784195385481
INFO::----step102000----
INFO::hq_ratio: 0.7226
INFO::entropy: 2.7347601971866715
INFO::----step103000----
INFO::hq_ratio: 0.7519
INFO::entropy: 2.7324516477926553
INFO::----step104000----
INFO::hq_ratio: 0.4933
INFO::entropy: 2.6031572356918495
INFO::----step105000----
INFO::hq_ratio: 0.5647
INFO::entropy: 2.7415247876569806
INFO::----step106000----
INFO::hq_ratio: 0.6141
INFO::entropy: 2.705321130968751
INFO::----step107000----
INFO::hq_ratio: 0.6283
INFO::entropy: 2.6664691522109822
INFO::----step108000----
INFO::hq_ratio: 0.7234
INFO::entropy: 2.7779603620294044
INFO::----step109000----
INFO::hq_ratio: 0.7105
INFO::entropy: 2.7963748013756082
INFO::----step110000----
INFO::hq_ratio: 0.7031
INFO::entropy: 2.7817836387719077
INFO::----step111000----
INFO::hq_ratio: 0.7139
INFO::entropy: 2.784726311207326
INFO::----step112000----
INFO::hq_ratio: 0.6961
INFO::entropy: 2.7926951889445903
INFO::----step113000----
INFO::hq_ratio: 0.7126
INFO::entropy: 2.7953878242511236
INFO::----step114000----
INFO::hq_ratio: 0.7108
INFO::entropy: 2.800250176992554
INFO::----step115000----
INFO::hq_ratio: 0.7439
INFO::entropy: 2.817645852489833
INFO::----step116000----
INFO::hq_ratio: 0.7259
INFO::entropy: 2.7620514654927786
INFO::----step117000----
INFO::hq_ratio: 0.7078
INFO::entropy: 2.7971783424955556
INFO::----step118000----
INFO::hq_ratio: 0.7142
INFO::entropy: 2.7852605607442116
INFO::----step119000----
INFO::hq_ratio: 0.7219
INFO::entropy: 2.7834433050457292
INFO::----step120000----
INFO::hq_ratio: 0.7039
INFO::entropy: 2.793093884552209
INFO::----step121000----
INFO::hq_ratio: 0.6989
INFO::entropy: 2.761802540792031
INFO::----step122000----
INFO::hq_ratio: 0.7415
INFO::entropy: 2.815090258942089
INFO::----step123000----
INFO::hq_ratio: 0.711
INFO::entropy: 2.826748000231161
INFO::----step124000----
INFO::hq_ratio: 0.6536
INFO::entropy: 2.7431168958237504
load 100000 samples.
load 2000 samples.
Traceback (most recent call last):
  File "trainer.py", line 271, in <module>
    trainer.train()
  File "trainer.py", line 134, in train
    final_reward, adv = model_stud.get_final_reward()
  File "/users/hzhang2/haowen/GitHub/autoLoss/models/gan_grid.py", line 339, in get_final_reward
    reward = self.best_hq_ratio * self.reward_c
AttributeError: 'Gan_grid' object has no attribute 'reward_c'
